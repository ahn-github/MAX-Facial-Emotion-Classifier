{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Emotion Classifier Example\n",
    "\n",
    "In this simple example, we show how the [MAX Facial Emotion Classifier](https://developer.ibm.com/exchanges/models/all/max-facial-emotion-classifier) model can be used to detect human faces in an image and predict the emotional state of each face. Additionally, we show how to use the returned bounding box coordinates for each face to visualize the bounding boxes and emotion predictions on the original input image.\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "The notebook calls the `MAX Facial Emotion Classifier` microservice, which must be running. You can either use the [hosted demo instance](http://max-facial-emotion-classifier.max.us-south.containers.appdomain.cloud), or follow the instructions for [deploying the microservice locally from the Dockerhub image](https://github.com/IBM/MAX-Facial-Emotion-Classifier#deploy-from-docker-hub). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook requires matplotlib, Pillow and requests\n",
    "# You only need to run the line below to install these if you don't already have them installed\n",
    "\n",
    "! pip install -q matplotlib Pillow requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This url must point to a running instance of the model microservice\n",
    "# By default this assumes you are using the hosted demo instance:\n",
    "\n",
    "url = 'http://max-facial-emotion-classifier.max.us-south.containers.appdomain.cloud/model/predict'\n",
    "# Comment the line above and uncomment the following line if you are running the model microservice locally\n",
    "# url = 'http://localhost:5000/model/predict'\n",
    "\n",
    "def call_model(input_img):\n",
    "    \"\"\"\n",
    "    Takes in input image file path, posts the image to the model and returns face bboxes and emotion predictions\n",
    "    \"\"\"\n",
    "    files = {'image': ('image.jpg', open(input_img, 'rb'), 'images/jpeg') }\n",
    "    r = requests.post(url, files=files).json()\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Visualizing the test image\n",
    "\n",
    "First we load the image with Pillow and display the image in our notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'assets/group.jpeg'\n",
    "image = Image.open(img_path)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Call Model to detect faces and predict emotions for each face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the emotion classifier model\n",
    "model_response = call_model(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Visualize model response\n",
    "\n",
    "The model returns JSON containing a `predictions` field which is an array of JSON objects, one for each face detected in the image. For each face, the bounding box coordinates are contained in the `detection_box` field, while the emotion classification results are contained in the `emotion_predictions` field.\n",
    "\n",
    "The bounding box coordinates are given in the format `[ymin, xmin, ymax, xmax]`, where each coordinate is _normalized_ by the appropriate image dimension (height for `y` or width for `x`). Each coordinate is therefore in the range `[0, 1]`. In order to use these coordinates to display the bounding boxes, we must first map them back to the same range as the original image, so that they become pixel coordinates.\n",
    "\n",
    "From each face's `emotion_predictions`, we will use the first entry in the array which will be the emotion class with the highest predicted probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the model results - there should be 4 entries in the `predictions` array\n",
    "import json\n",
    "print(json.dumps(model_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We display bounding boxes and the emotion class with highest predicted probability for each detected face\n",
    "\n",
    "# Get the image height and width\n",
    "image_width, image_height = image.size\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "# Set larger figure size\n",
    "fig.set_dpi(600)\n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "\n",
    "# Set up the color of the bounding boxes and text\n",
    "color = '#00FF00'\n",
    "# For each face, draw the bounding box and predicted emotion class together with the probability\n",
    "for prediction in model_response['predictions']:\n",
    "    bbox = prediction['detection_box']\n",
    "    # Unpack the coordinate values\n",
    "    y1, x1, y2, x2 = bbox\n",
    "    # Map the normalized coordinates to pixel values: scale by image height for 'y' and image width for 'x'\n",
    "    y1 *= image_height\n",
    "    y2 *= image_height\n",
    "    x1 *= image_width\n",
    "    x2 *= image_width\n",
    "    emotion_prediction = prediction['emotion_predictions'][0]\n",
    "    # Format the emotion class probability for display\n",
    "    emotion_probability = '{0:.4f}'.format(emotion_prediction['probability'])\n",
    "    # Format the emotion class label for display\n",
    "    emotion_label = '{}:'.format(emotion_prediction['label'])\n",
    "    emotion_label = emotion_label.capitalize()\n",
    "    # Create the bounding box rectangle - we need the base point (x, y) and the width and height of the rectangle\n",
    "    rectangle = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=1, edgecolor=color, facecolor='none')\n",
    "    ax.add_patch(rectangle)\n",
    "    # Plot the emotion class and probability text\n",
    "    plt.text(x1, y1 - 25, emotion_label, fontsize=4, color=color, fontweight='bold')\n",
    "    plt.text(x1, y1 - 5, emotion_probability, fontsize=4, color=color, fontweight='bold')\n",
    "    \n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
